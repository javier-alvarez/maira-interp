{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": "# MAIRA-2 vs Qwen2-VL-2B Comparison for Memory-Constrained GPUs\n\n**üéØ Two options for different GPU memory constraints:**\n\n- **Qwen2-VL-2B**: ~4GB VRAM, works on any GPU, general vision-language model\n- **MAIRA-2**: ~15GB VRAM, specialized for radiology, state-of-the-art medical AI\n\n**Setup:**\n1. Runtime > Change runtime type > GPU > Save\n2. Get HF token: https://huggingface.co/settings/tokens\n3. Choose your model based on available GPU memory"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-setup"
   },
   "outputs": [],
   "source": "# Check environment and recommend model\nimport torch\nimport subprocess\n\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu_props = torch.cuda.get_device_properties(0)\n    gpu_memory_gb = gpu_props.total_memory / 1024**3\n    print(f\"GPU: {gpu_props.name}\")\n    print(f\"Memory: {gpu_memory_gb:.1f}GB\")\n    \n    print(f\"\\nüéØ Model Recommendations:\")\n    if gpu_memory_gb < 6:\n        print(\"‚ùå Insufficient memory for vision-language models\")\n        print(\"Try: Runtime > Change runtime type > Select different GPU\")\n    elif gpu_memory_gb < 14:\n        print(f\"‚úÖ Qwen2-VL-2B (4GB) - RECOMMENDED for {gpu_memory_gb:.1f}GB GPU\")\n        print(\"‚ö†Ô∏è  MAIRA-2 (15GB) - Will likely fail with OOM\")\n        recommended_model = \"qwen2-vl-2b\"\n    elif gpu_memory_gb < 16:\n        print(f\"‚ö†Ô∏è  Both models possible but MAIRA-2 will be tight\")\n        print(\"üéØ Try Qwen2-VL-2B first, then MAIRA-2 if you want\")\n        recommended_model = \"qwen2-vl-2b\"  # Safer choice\n    else:\n        print(\"‚úÖ Both models will work fine\")\n        recommended_model = \"maira-2\"  # Can afford the medical specialist\n        \n    print(f\"\\nüîß Recommended: {recommended_model}\")\nelse:\n    print(\"‚ùå No GPU! Go to Runtime > Change runtime type > GPU\")\n    recommended_model = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-minimal"
   },
   "outputs": [],
   "source": "# Install packages for both models - latest versions\n!pip install -q torch torchvision\n!pip install -q transformers>=4.45.0 accelerate  # Need latest for both models\n!pip install -q pillow matplotlib requests\nprint(\"‚úÖ Latest packages installed for both models\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory-utils"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def aggressive_cleanup():\n",
    "    \"\"\"Aggressive memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        # Force garbage collection multiple times\n",
    "        for _ in range(3):\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check GPU memory with cleanup\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        free = total - allocated\n",
    "        print(f\"GPU: {allocated:.1f}GB used, {free:.1f}GB free, {total:.1f}GB total\")\n",
    "        return free > 1.0  # Need at least 1GB free\n",
    "    return False\n",
    "\n",
    "# Set memory optimization environment variables BEFORE importing transformers\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256,expandable_segments:True'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Initial cleanup\n",
    "aggressive_cleanup()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth-minimal"
   },
   "outputs": [],
   "source": [
    "# Authentication (essential)\n",
    "from getpass import getpass\n",
    "\n",
    "hf_token = getpass(\"Hugging Face token: \")\n",
    "os.environ['HF_TOKEN'] = hf_token\n",
    "\n",
    "# Quick auth test\n",
    "try:\n",
    "    from huggingface_hub import HfApi\n",
    "    api = HfApi()\n",
    "    user = api.whoami(token=hf_token)\n",
    "    print(f\"‚úÖ Authenticated as: {user['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Auth failed: {e}\")\n",
    "    \n",
    "del api, user  # Cleanup immediately\n",
    "aggressive_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get-repo"
   },
   "outputs": [],
   "source": "# Dual-model wrapper for MAIRA-2 and Qwen2-VL-2B\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nclass DualVisionLanguageModel:\n    \"\"\"Unified wrapper for MAIRA-2 and Qwen2-VL-2B with minimal differences\"\"\"\n    \n    def __init__(self, model_name=\"qwen2-vl-2b\"):\n        self.model_name = model_name\n        self.model = None\n        self.processor = None\n        \n        # Model configurations\n        self.configs = {\n            \"qwen2-vl-2b\": {\n                \"model_id\": \"Qwen/Qwen2-VL-2B-Instruct\",\n                \"memory_gb\": 4,\n                \"needs_token\": False,\n                \"trust_remote_code\": True,\n                \"torch_dtype\": torch.bfloat16,\n                \"max_new_tokens\": 50\n            },\n            \"maira-2\": {\n                \"model_id\": \"microsoft/maira-2\", \n                \"memory_gb\": 15,\n                \"needs_token\": True,\n                \"trust_remote_code\": True,\n                \"torch_dtype\": torch.float16,\n                \"max_new_tokens\": 15\n            }\n        }\n        \n        if model_name not in self.configs:\n            raise ValueError(f\"Unsupported model: {model_name}. Use 'qwen2-vl-2b' or 'maira-2'\")\n            \n        self.config = self.configs[model_name]\n        \n    def load_model_cautiously(self):\n        \"\"\"Load model with appropriate optimizations\"\"\"\n        print(f\"üîÑ Loading {self.model_name} ({self.config['memory_gb']}GB)...\")\n        \n        # Check memory before loading\n        if not check_gpu_memory():\n            print(\"‚ùå Insufficient GPU memory\")\n            return False\n            \n        try:\n            # Get token if needed\n            token = os.environ.get('HF_TOKEN') if self.config['needs_token'] else None\n            \n            print(\"Loading processor...\")\n            self.processor = AutoProcessor.from_pretrained(\n                self.config['model_id'],\n                trust_remote_code=self.config['trust_remote_code'],\n                token=token\n            )\n            \n            aggressive_cleanup()\n            \n            print(\"Loading model...\")\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.config['model_id'],\n                trust_remote_code=self.config['trust_remote_code'],\n                token=token,\n                torch_dtype=self.config['torch_dtype'],\n                low_cpu_mem_usage=True,\n                device_map=\"auto\"\n            )\n            \n            # Set to eval mode\n            if hasattr(self.model, 'eval'):\n                self.model.eval()\n                \n            aggressive_cleanup()\n            \n            print(f\"‚úÖ {self.model_name} loaded successfully!\")\n            check_gpu_memory()\n            return True\n            \n        except Exception as e:\n            print(f\"‚ùå Model loading failed: {e}\")\n            self.cleanup()\n            return False\n    \n    def generate_report(self, image_path_or_pil, prompt=None):\n        \"\"\"Generate report with model-appropriate prompting\"\"\"\n        if self.model is None or self.processor is None:\n            print(\"‚ùå Model not loaded\")\n            return None\n            \n        try:\n            # Load and prepare image\n            if isinstance(image_path_or_pil, str):\n                if image_path_or_pil.startswith('http'):\n                    response = requests.get(image_path_or_pil)\n                    image = Image.open(BytesIO(response.content))\n                else:\n                    image = Image.open(image_path_or_pil)\n            else:\n                image = image_path_or_pil\n                \n            # Convert to RGB for consistency\n            if image.mode != 'RGB':\n                image = image.convert('RGB')\n            \n            # Model-specific prompting\n            if self.model_name == \"qwen2-vl-2b\":\n                return self._generate_qwen(image, prompt)\n            else:  # maira-2\n                return self._generate_maira(image, prompt)\n                \n        except Exception as e:\n            print(f\"‚ùå Generation failed: {e}\")\n            aggressive_cleanup()\n            return None\n    \n    def _generate_qwen(self, image, prompt=None):\n        \"\"\"Generate using Qwen2-VL-2B format\"\"\"\n        if prompt is None:\n            prompt = \"Analyze this chest X-ray image. Describe any abnormalities you observe.\"\n            \n        # Qwen2-VL conversation format\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": image},\n                    {\"type\": \"text\", \"text\": prompt}\n                ]\n            }\n        ]\n        \n        # Prepare inputs\n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        \n        inputs = self.processor(\n            text=[text], \n            images=[image], \n            return_tensors=\"pt\"\n        )\n        \n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        \n        # Generate\n        with torch.no_grad():\n            output = self.model.generate(\n                **inputs,\n                max_new_tokens=self.config['max_new_tokens'],\n                do_sample=False,\n                temperature=None,\n                top_p=None\n            )\n        \n        # Decode\n        prompt_len = inputs['input_ids'].shape[1]\n        generated_tokens = output[0][prompt_len:]\n        response = self.processor.tokenizer.decode(\n            generated_tokens, skip_special_tokens=True\n        )\n        \n        # Cleanup\n        del inputs, output, generated_tokens\n        aggressive_cleanup()\n        \n        return response.strip()\n    \n    def _generate_maira(self, image, prompt=None):\n        \"\"\"Generate using MAIRA-2 format\"\"\"\n        if prompt is None:\n            prompt = \"Describe this chest X-ray briefly.\"\n            \n        # Convert to grayscale for MAIRA-2 (medical standard)\n        if image.mode != 'L':\n            image = image.convert('L')\n        \n        # MAIRA-2 conversation format\n        conversation = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": prompt}\n                ]\n            }\n        ]\n        \n        prompt_text = self.processor.apply_chat_template(\n            conversation, add_generation_prompt=True\n        )\n        \n        inputs = self.processor(\n            text=prompt_text,\n            images=[image],\n            return_tensors=\"pt\"\n        )\n        \n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        \n        # Generate\n        with torch.no_grad():\n            output = self.model.generate(\n                **inputs,\n                max_new_tokens=self.config['max_new_tokens'],\n                do_sample=False,\n                pad_token_id=self.processor.tokenizer.eos_token_id\n            )\n        \n        # Decode\n        prompt_len = inputs['input_ids'].shape[1]\n        generated_tokens = output[0][prompt_len:]\n        response = self.processor.tokenizer.decode(\n            generated_tokens, skip_special_tokens=True\n        )\n        \n        # Cleanup\n        del inputs, output, generated_tokens\n        aggressive_cleanup()\n        \n        return response.strip()\n    \n    def cleanup(self):\n        \"\"\"Clean up model from memory\"\"\"\n        if self.model is not None:\n            del self.model\n            self.model = None\n        if self.processor is not None:\n            del self.processor\n            self.processor = None\n        aggressive_cleanup()\n\nprint(\"‚úÖ Dual-model wrapper created\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-lightweight"
   },
   "outputs": [],
   "source": "# Choose and initialize your model\nprint(\"üéØ Model Selection\")\nprint(\"1. Qwen2-VL-2B: ~4GB, works on any GPU, general vision-language\")\nprint(\"2. MAIRA-2: ~15GB, medical specialist, needs HF token\")\n\n# Auto-select based on earlier recommendation, but allow manual override\nmodel_choice = input(f\"Choose model (1 for Qwen2-VL-2B, 2 for MAIRA-2) [default: 1]: \").strip()\n\nif model_choice == \"2\":\n    model_name = \"maira-2\"\n    print(\"Selected: MAIRA-2 (medical specialist)\")\n    \n    # Check if token is needed\n    if 'HF_TOKEN' not in os.environ:\n        from getpass import getpass\n        hf_token = getpass(\"Enter your HF token (required for MAIRA-2): \")\n        os.environ['HF_TOKEN'] = hf_token\nelse:\n    model_name = \"qwen2-vl-2b\" \n    print(\"Selected: Qwen2-VL-2B (general vision-language)\")\n\n# Initialize model\nprint(f\"\\nüöÄ Initializing {model_name}...\")\nvisualizer = DualVisionLanguageModel(model_name)\n\nsuccess = visualizer.load_model_cautiously()\n\nif success:\n    print(f\"üéâ {model_name} loaded successfully!\")\n    print(\"Ready for image analysis.\")\nelse:\n    print(f\"‚ùå Failed to load {model_name}\")\n    if model_name == \"maira-2\":\n        print(\"Try switching to Qwen2-VL-2B (option 1)\")\n    else:\n        print(\"Check your GPU memory or try restarting runtime\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": "# Test with sample chest X-ray on both models\nif visualizer.model is not None:\n    print(f\"üîç Testing {visualizer.model_name} with sample chest X-ray...\")\n    \n    # Use the same sample image for comparison\n    sample_url = \"https://openi.nlm.nih.gov/imgs/512/145/145/CXR145_IM-0290-1001.png\"\n    \n    # Test prompts\n    prompts = [\n        \"Describe this chest X-ray image briefly.\",\n        \"What abnormalities do you see in this chest X-ray?\",\n        \"Analyze this medical image and report your findings.\"\n    ]\n    \n    for i, prompt in enumerate(prompts, 1):\n        print(f\"\\n--- Test {i}: {prompt} ---\")\n        \n        result = visualizer.generate_report(sample_url, prompt)\n        \n        if result:\n            print(f\"üìÑ {visualizer.model_name} Response:\")\n            print(\"=\" * 50)\n            print(result)\n            print(\"=\" * 50)\n        else:\n            print(\"‚ùå Generation failed for this prompt\")\n            \n        # Check memory after each test\n        check_gpu_memory()\n    \n    print(f\"\\n‚úÖ {visualizer.model_name} testing complete!\")\n    \n    # Show sample image for reference\n    try:\n        import requests\n        from PIL import Image\n        from io import BytesIO\n        response = requests.get(sample_url)\n        img = Image.open(BytesIO(response.content))\n        print(\"\\nüì∏ Sample chest X-ray used for testing:\")\n        display(img.resize((256, 256)))\n    except:\n        print(\"üì∏ Sample image URL: \" + sample_url)\n        \nelse:\n    print(\"‚ùå Model not loaded - cannot test\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-minimal"
   },
   "outputs": [],
   "source": [
    "# Test with minimal example\n",
    "if visualizer.model is not None:\n",
    "    print(\"üîç Testing with sample chest X-ray...\")\n",
    "    \n",
    "    # Use a small sample image URL\n",
    "    sample_url = \"https://openi.nlm.nih.gov/imgs/512/145/145/CXR145_IM-0290-1001.png\"\n",
    "    \n",
    "    # Generate minimal report\n",
    "    prompt = \"Describe this chest X-ray briefly.\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"Generating...\")\n",
    "    \n",
    "    result = visualizer.generate_minimal_report(sample_url, prompt)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\nüìÑ Generated Report:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(result)\n",
    "        print(\"=\" * 40)\n",
    "        print(\"\\n‚úÖ Minimal demo successful!\")\n",
    "    else:\n",
    "        print(\"‚ùå Generation failed\")\n",
    "        \n",
    "    check_gpu_memory()\nelse:\n",
    "    print(\"‚ùå Model not loaded - cannot test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "try-your-image"
   },
   "outputs": [],
   "source": [
    "# Optional: Try with your own image\n",
    "if visualizer.model is not None:\n",
    "    print(\"üì∏ You can now try with your own chest X-ray:\")\n",
    "    print(\"1. Upload an image file to Colab (click folder icon üóÇÔ∏è)\")\n",
    "    print(\"2. Update the image_path below\")\n",
    "    print(\"3. Run this cell\")\n",
    "    \n",
    "    # CHANGE THIS PATH to your uploaded image\n",
    "    image_path = \"your_image.png\"  # Replace with your image filename\n",
    "    \n",
    "    # Check if image exists\n",
    "    if os.path.exists(image_path):\n",
    "        print(f\"Processing {image_path}...\")\n",
    "        \n",
    "        custom_prompt = \"What abnormalities do you see in this chest X-ray?\"\n",
    "        result = visualizer.generate_minimal_report(image_path, custom_prompt)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"\\nüìÑ Report for {image_path}:\")\n",
    "            print(\"=\" * 40)\n",
    "            print(result)\n",
    "            print(\"=\" * 40)\n",
    "        else:\n",
    "            print(\"‚ùå Failed to process your image\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Image {image_path} not found.\")\n",
    "        print(\"Upload an image first, then update the image_path variable.\")\nelse:\n",
    "    print(\"‚ùå Model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "final-cleanup"
   },
   "outputs": [],
   "source": "## üìä Model Comparison Summary\n\n**This notebook lets you compare two vision-language models:**\n\n### üî¨ **Qwen2-VL-2B** (Recommended for Colab)\n- **Memory**: ~4GB (fits any GPU)\n- **Speed**: Fast inference  \n- **Domain**: General vision-language\n- **Token**: Not required\n- **Output**: 50 tokens, good general analysis\n\n### üè• **MAIRA-2** (Medical Specialist)\n- **Memory**: ~15GB (tight fit on free Colab)\n- **Speed**: Slower due to size\n- **Domain**: Radiology specialist\n- **Token**: Required (HF account + MAIRA-2 access)\n- **Output**: 15 tokens, focused medical analysis\n\n### üéØ **Key Differences in Code:**\n1. **Input format**: Qwen uses RGB, MAIRA uses grayscale\n2. **Chat templates**: Different conversation structures\n3. **Token requirements**: MAIRA needs authentication\n4. **Memory optimization**: MAIRA uses float16, Qwen uses bfloat16\n\n### üí° **Recommendations:**\n- **Start with Qwen2-VL-2B** to validate your workflow\n- **Switch to MAIRA-2** for actual medical applications\n- **Use this notebook** to test both and compare outputs\n- **Qwen2-VL-2B is perfect** for prototyping and development\n\nBoth models use the same inference pipeline with minimal differences!"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üìã Summary\n",
    "\n",
    "**This lightweight notebook:**\n",
    "- ‚úÖ Works on 14.7GB GPUs (Colab T4)\n",
    "- ‚úÖ Uses aggressive memory optimization\n",
    "- ‚úÖ Generates minimal but functional reports\n",
    "- ‚úÖ Handles OOM gracefully\n",
    "\n",
    "**Limitations:**\n",
    "- Only generates ~15 tokens (vs 100+ in full version)\n",
    "- No attention visualizations (would require too much memory)\n",
    "- Single image only (no lateral/prior images)\n",
    "- Half precision (might affect quality slightly)\n",
    "\n",
    "**For full features:**\n",
    "- Use Colab Pro (guaranteed A100/V100 with 40GB+)\n",
    "- Try Kaggle Notebooks (sometimes more generous)\n",
    "- Use Paperspace or other cloud platforms\n",
    "\n",
    "**This demo proves MAIRA-2 can run on your hardware!** üéâ"
   ]
  }
 ]
}