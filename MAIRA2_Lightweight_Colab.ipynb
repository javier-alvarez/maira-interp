{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# MAIRA-2 Lightweight Demo for Memory-Constrained GPUs\n",
        "\n",
        "**üéØ Designed specifically for 14-15GB GPUs (Colab T4)**\n",
        "\n",
        "This notebook uses aggressive memory optimizations to run MAIRA-2 on limited GPU memory.\n",
        "\n",
        "**Setup:**\n",
        "1. Runtime > Change runtime type > GPU > Save\n",
        "2. Get HF token: https://huggingface.co/settings/tokens\n",
        "3. Request MAIRA-2 access: https://huggingface.co/microsoft/maira-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check-setup"
      },
      "outputs": [],
      "source": [
        "# Check environment\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_props = torch.cuda.get_device_properties(0)\n",
        "    gpu_memory_gb = gpu_props.total_memory / 1024**3\n",
        "    print(f\"GPU: {gpu_props.name}\")\n",
        "    print(f\"Memory: {gpu_memory_gb:.1f}GB\")\n",
        "    \n",
        "    if gpu_memory_gb < 14:\n",
        "        print(\"‚ùå GPU has insufficient memory for MAIRA-2\")\n",
        "        print(\"Try: Runtime > Change runtime type > Select different GPU\")\n",
        "    elif gpu_memory_gb < 16:\n",
        "        print(\"‚ö†Ô∏è  GPU memory is at the minimum. Using ultra-lightweight mode.\")\n",
        "    else:\n",
        "        print(\"‚úÖ GPU memory sufficient\")\nelse:\n",
        "    print(\"‚ùå No GPU! Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-minimal"
      },
      "outputs": [],
      "source": [
        "# Install only essential packages\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q transformers==4.44.0 accelerate\n",
        "!pip install -q pillow matplotlib requests\n",
        "print(\"‚úÖ Packages installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memory-utils"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def aggressive_cleanup():\n",
        "    \"\"\"Aggressive memory cleanup\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "        # Force garbage collection multiple times\n",
        "        for _ in range(3):\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "def check_gpu_memory():\n",
        "    \"\"\"Check GPU memory with cleanup\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        free = total - allocated\n",
        "        print(f\"GPU: {allocated:.1f}GB used, {free:.1f}GB free, {total:.1f}GB total\")\n",
        "        return free > 1.0  # Need at least 1GB free\n",
        "    return False\n",
        "\n",
        "# Set memory optimization environment variables BEFORE importing transformers\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256,expandable_segments:True'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "# Initial cleanup\n",
        "aggressive_cleanup()\n",
        "check_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auth-minimal"
      },
      "outputs": [],
      "source": [
        "# Authentication (essential)\n",
        "from getpass import getpass\n",
        "\n",
        "hf_token = getpass(\"Hugging Face token: \")\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "# Quick auth test\n",
        "try:\n",
        "    from huggingface_hub import HfApi\n",
        "    api = HfApi()\n",
        "    user = api.whoami(token=hf_token)\n",
        "    print(f\"‚úÖ Authenticated as: {user['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Auth failed: {e}\")\n",
        "    \n",
        "del api, user  # Cleanup immediately\n",
        "aggressive_cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get-repo"
      },
      "outputs": [],
      "source": [
        "# Get the lightweight visualizer\n",
        "!git clone -q https://github.com/javier-alvarez/maira-interp.git\n",
        "%cd maira-interp\n",
        "\n",
        "# Verify we have the lightweight version\n",
        "!ls lightweight_visualizer.py\n",
        "print(\"‚úÖ Repository cloned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-lightweight"
      },
      "outputs": [],
      "source": [
        "# Create ultra-lightweight MAIRA-2 wrapper\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "class UltraLightMAIRA2:\n",
        "    \"\"\"Ultra-lightweight MAIRA-2 wrapper for memory-constrained environments\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.processor = None\n",
        "        \n",
        "    def load_model_cautiously(self):\n",
        "        \"\"\"Load model with maximum memory optimizations\"\"\"\n",
        "        print(\"üîÑ Loading MAIRA-2 with extreme memory optimization...\")\n",
        "        \n",
        "        # Check memory before loading\n",
        "        if not check_gpu_memory():\n",
        "            print(\"‚ùå Insufficient GPU memory\")\n",
        "            return False\n",
        "            \n",
        "        try:\n",
        "            # Load with maximum memory savings\n",
        "            print(\"Loading processor...\")\n",
        "            self.processor = AutoProcessor.from_pretrained(\n",
        "                \"microsoft/maira-2\",\n",
        "                trust_remote_code=True,\n",
        "                token=os.environ.get('HF_TOKEN'),\n",
        "                torch_dtype=torch.float16  # Half precision\n",
        "            )\n",
        "            \n",
        "            aggressive_cleanup()\n",
        "            \n",
        "            print(\"Loading model...\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"microsoft/maira-2\",\n",
        "                trust_remote_code=True,\n",
        "                token=os.environ.get('HF_TOKEN'),\n",
        "                torch_dtype=torch.float16,  # Half precision\n",
        "                low_cpu_mem_usage=True,     # Reduce CPU memory\n",
        "                device_map=\"auto\"           # Automatic device placement\n",
        "            )\n",
        "            \n",
        "            # Additional memory optimization\n",
        "            if hasattr(self.model, 'eval'):\n",
        "                self.model.eval()  # Set to eval mode\n",
        "                \n",
        "            aggressive_cleanup()\n",
        "            \n",
        "            print(\"‚úÖ Model loaded successfully!\")\n",
        "            check_gpu_memory()\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Model loading failed: {e}\")\n",
        "            self.cleanup()\n",
        "            return False\n",
        "    \n",
        "    def generate_minimal_report(self, image_path_or_pil, prompt=\"What do you see?\"):\n",
        "        \"\"\"Generate minimal report with maximum memory efficiency\"\"\"\n",
        "        if self.model is None or self.processor is None:\n",
        "            print(\"‚ùå Model not loaded\")\n",
        "            return None\n",
        "            \n",
        "        try:\n",
        "            # Load image\n",
        "            if isinstance(image_path_or_pil, str):\n",
        "                if image_path_or_pil.startswith('http'):\n",
        "                    response = requests.get(image_path_or_pil)\n",
        "                    image = Image.open(BytesIO(response.content))\n",
        "                else:\n",
        "                    image = Image.open(image_path_or_pil)\n",
        "            else:\n",
        "                image = image_path_or_pil\n",
        "                \n",
        "            # Convert to grayscale to save memory\n",
        "            if image.mode != 'L':\n",
        "                image = image.convert('L')\n",
        "            \n",
        "            # Process inputs with minimal context\n",
        "            conversation = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"image\"},\n",
        "                        {\"type\": \"text\", \"text\": prompt}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "            \n",
        "            prompt_text = self.processor.apply_chat_template(\n",
        "                conversation, add_generation_prompt=True\n",
        "            )\n",
        "            \n",
        "            inputs = self.processor(\n",
        "                text=prompt_text,\n",
        "                images=[image],\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            \n",
        "            # Move to GPU\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Generate with minimal tokens\n",
        "            with torch.no_grad():  # Save memory\n",
        "                output = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=15,  # Very minimal\n",
        "                    do_sample=False,    # Deterministic\n",
        "                    temperature=None,   # No sampling\n",
        "                    top_p=None,\n",
        "                    pad_token_id=self.processor.tokenizer.eos_token_id\n",
        "                )\n",
        "            \n",
        "            # Decode response\n",
        "            prompt_len = inputs['input_ids'].shape[1]\n",
        "            generated_tokens = output[0][prompt_len:]\n",
        "            response = self.processor.tokenizer.decode(\n",
        "                generated_tokens, skip_special_tokens=True\n",
        "            )\n",
        "            \n",
        "            # Cleanup\n",
        "            del inputs, output, generated_tokens\n",
        "            aggressive_cleanup()\n",
        "            \n",
        "            return response.strip()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Generation failed: {e}\")\n",
        "            aggressive_cleanup()\n",
        "            return None\n",
        "    \n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up model from memory\"\"\"\n",
        "        if self.model is not None:\n",
        "            del self.model\n",
        "            self.model = None\n",
        "        if self.processor is not None:\n",
        "            del self.processor\n",
        "            self.processor = None\n",
        "        aggressive_cleanup()\n",
        "\n",
        "print(\"‚úÖ Ultra-lightweight wrapper created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-model"
      },
      "outputs": [],
      "source": [
        "# Initialize and load model\n",
        "visualizer = UltraLightMAIRA2()\n",
        "\n",
        "print(\"Starting model loading...\")\n",
        "print(\"This will take 5-10 minutes on first run (downloading ~13GB)\")\n",
        "\n",
        "success = visualizer.load_model_cautiously()\n",
        "\n",
        "if success:\n",
        "    print(\"üéâ MAIRA-2 loaded successfully!\")\n",
        "    print(\"Ready for minimal demonstration.\")\nelse:\n",
        "    print(\"‚ùå Failed to load MAIRA-2\")\n",
        "    print(\"Your GPU doesn't have enough memory.\")\n",
        "    print(\"Try Colab Pro or Kaggle Notebooks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-minimal"
      },
      "outputs": [],
      "source": [
        "# Test with minimal example\n",
        "if visualizer.model is not None:\n",
        "    print(\"üîç Testing with sample chest X-ray...\")\n",
        "    \n",
        "    # Use a small sample image URL\n",
        "    sample_url = \"https://openi.nlm.nih.gov/imgs/512/145/145/CXR145_IM-0290-1001.png\"\n",
        "    \n",
        "    # Generate minimal report\n",
        "    prompt = \"Describe this chest X-ray briefly.\"\n",
        "    \n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(\"Generating...\")\n",
        "    \n",
        "    result = visualizer.generate_minimal_report(sample_url, prompt)\n",
        "    \n",
        "    if result:\n",
        "        print(f\"\\nüìÑ Generated Report:\")\n",
        "        print(\"=\" * 40)\n",
        "        print(result)\n",
        "        print(\"=\" * 40)\n",
        "        print(\"\\n‚úÖ Minimal demo successful!\")\n",
        "    else:\n",
        "        print(\"‚ùå Generation failed\")\n",
        "        \n",
        "    check_gpu_memory()\nelse:\n",
        "    print(\"‚ùå Model not loaded - cannot test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "try-your-image"
      },
      "outputs": [],
      "source": [
        "# Optional: Try with your own image\n",
        "if visualizer.model is not None:\n",
        "    print(\"üì∏ You can now try with your own chest X-ray:\")\n",
        "    print(\"1. Upload an image file to Colab (click folder icon üóÇÔ∏è)\")\n",
        "    print(\"2. Update the image_path below\")\n",
        "    print(\"3. Run this cell\")\n",
        "    \n",
        "    # CHANGE THIS PATH to your uploaded image\n",
        "    image_path = \"your_image.png\"  # Replace with your image filename\n",
        "    \n",
        "    # Check if image exists\n",
        "    if os.path.exists(image_path):\n",
        "        print(f\"Processing {image_path}...\")\n",
        "        \n",
        "        custom_prompt = \"What abnormalities do you see in this chest X-ray?\"\n",
        "        result = visualizer.generate_minimal_report(image_path, custom_prompt)\n",
        "        \n",
        "        if result:\n",
        "            print(f\"\\nüìÑ Report for {image_path}:\")\n",
        "            print(\"=\" * 40)\n",
        "            print(result)\n",
        "            print(\"=\" * 40)\n",
        "        else:\n",
        "            print(\"‚ùå Failed to process your image\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Image {image_path} not found.\")\n",
        "        print(\"Upload an image first, then update the image_path variable.\")\nelse:\n",
        "    print(\"‚ùå Model not loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final-cleanup"
      },
      "outputs": [],
      "source": [
        "# Final cleanup\n",
        "print(\"üßπ Cleaning up memory...\")\n",
        "\n",
        "if 'visualizer' in locals():\n",
        "    visualizer.cleanup()\n",
        "    del visualizer\n",
        "\n",
        "aggressive_cleanup()\n",
        "print(\"‚úÖ Memory cleaned\")\n",
        "check_gpu_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## üìã Summary\n",
        "\n",
        "**This lightweight notebook:**\n",
        "- ‚úÖ Works on 14.7GB GPUs (Colab T4)\n",
        "- ‚úÖ Uses aggressive memory optimization\n",
        "- ‚úÖ Generates minimal but functional reports\n",
        "- ‚úÖ Handles OOM gracefully\n",
        "\n",
        "**Limitations:**\n",
        "- Only generates ~15 tokens (vs 100+ in full version)\n",
        "- No attention visualizations (would require too much memory)\n",
        "- Single image only (no lateral/prior images)\n",
        "- Half precision (might affect quality slightly)\n",
        "\n",
        "**For full features:**\n",
        "- Use Colab Pro (guaranteed A100/V100 with 40GB+)\n",
        "- Try Kaggle Notebooks (sometimes more generous)\n",
        "- Use Paperspace or other cloud platforms\n",
        "\n",
        "**This demo proves MAIRA-2 can run on your hardware!** üéâ"
      ]
    }
  ]
}