{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# MAIRA-2 Attention Visualization on Google Colab\n",
    "\n",
    "This notebook helps you visualize attention patterns in MAIRA-2 using free GPU resources.\n",
    "\n",
    "**‚ö†Ô∏è Important Setup:**\n",
    "1. Go to **Runtime > Change runtime type**\n",
    "2. Set **Hardware accelerator** to **GPU**\n",
    "3. Click **Save**\n",
    "4. You'll need a Hugging Face token from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check-gpu"
   },
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-check"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected! Go to Runtime > Change runtime type > GPU\")\n",
    "\n",
    "# Check available RAM\n",
    "result = subprocess.run(['free', '-h'], capture_output=True, text=True)\n",
    "print(\"\\nSystem Memory:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-deps"
   },
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision transformers accelerate\n",
    "!pip install pillow matplotlib tqdm requests\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo"
   },
   "source": [
    "## 3. Get the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/javier-alvarez/maira-interp.git\n",
    "%cd maira-interp\n",
    "\n",
    "# Verify files\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth-setup"
   },
   "source": [
    "## 4. Set Up Authentication\n",
    "\n",
    "**You need a Hugging Face token to access MAIRA-2:**\n",
    "1. Go to https://huggingface.co/settings/tokens\n",
    "2. Create a new token with 'Read' permissions\n",
    "3. Request access to MAIRA-2: https://huggingface.co/microsoft/maira-2\n",
    "4. Enter your token in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter your Hugging Face token (it will be hidden)\n",
    "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
    "os.environ['HF_TOKEN'] = hf_token\n",
    "\n",
    "print(\"‚úÖ Token set!\")\n",
    "\n",
    "# Test authentication\n",
    "from huggingface_hub import HfApi\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user = api.whoami(token=hf_token)\n",
    "    print(f\"‚úÖ Authenticated as: {user['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-import"
   },
   "source": [
    "## 5. Test Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-test"
   },
   "outputs": [],
   "source": [
    "# Test importing the visualizer\n",
    "try:\n",
    "    from attention_visualizer import MAIRA2AttentionVisualizer\n",
    "    print(\"‚úÖ Successfully imported MAIRA2AttentionVisualizer\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"Current directory:\", os.getcwd())\n",
    "    print(\"Files:\", os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "memory-opt"
   },
   "source": [
    "## 6. Memory Optimization Setup\n",
    "\n",
    "**Important:** MAIRA-2 requires ~15GB GPU memory. Free Colab GPUs have 16GB, so we need to be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory-setup"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and system memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def check_memory():\n",
    "    \"\"\"Check current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.1f}GB, Reserved: {reserved:.1f}GB, Total: {total:.1f}GB\")\n",
    "        return allocated, reserved, total\n",
    "    return 0, 0, 0\n",
    "\n",
    "# Clear memory before starting\n",
    "clear_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init-model"
   },
   "source": "## 7. Initialize MAIRA-2 with Memory Management\n\n**‚ö†Ô∏è Critical:** MAIRA-2 needs ~15GB. Your GPU has 14.7GB - we'll use aggressive optimizations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model-init"
   },
   "outputs": [],
   "source": "import torch\nprint(\"üöÄ Initializing MAIRA-2 Attention Visualizer...\")\nprint(\"This will download ~13GB of model weights on first run.\")\nprint(\"Please be patient - this may take 5-10 minutes.\")\n\n# Check available memory before loading\nallocated, reserved, total = check_memory()\nif total < 15:\n    print(f\"‚ö†Ô∏è  Warning: Only {total:.1f}GB GPU memory available. MAIRA-2 needs ~15GB.\")\n    print(\"Using memory optimization strategies...\")\n\ntry:\n    # Memory optimization for smaller GPUs\n    if total < 15:\n        print(\"üîß Loading with memory optimizations...\")\n        visualizer = MAIRA2AttentionVisualizer()\n        \n        # Move model to half precision to save memory\n        if hasattr(visualizer, 'model'):\n            visualizer.model = visualizer.model.half()\n        \n        print(\"‚úÖ MAIRA-2 loaded with memory optimizations!\")\n    else:\n        visualizer = MAIRA2AttentionVisualizer()\n        print(\"‚úÖ MAIRA-2 loaded successfully!\")\n    \n    check_memory()\n    \nexcept Exception as e:\n    print(f\"‚ùå Failed to load MAIRA-2: {e}\")\n    print(\"\\nüîß Trying aggressive memory optimization...\")\n    clear_memory()\n    \n    try:\n        # Last resort: try to load with minimal memory\n        print(\"Attempting to load with CPU offloading...\")\n        \n        # This is a fallback - may not work but worth trying\n        import os\n        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n        \n        visualizer = MAIRA2AttentionVisualizer()\n        print(\"‚úÖ MAIRA-2 loaded with CPU offloading!\")\n        check_memory()\n        \n    except Exception as e2:\n        print(f\"‚ùå All loading attempts failed: {e2}\")\n        print(\"\\nüí° Solutions:\")\n        print(\"1. Runtime > Restart Runtime and try again\")\n        print(\"2. Try Colab Pro for more GPU memory\")\n        print(\"3. Use a different GPU type if available\")\n        print(\"4. Try running at a different time when GPUs are less loaded\")\n        \n        # Set visualizer to None so notebook doesn't crash\n        visualizer = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample-images"
   },
   "source": [
    "## 8. Download Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-images"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Download sample chest X-ray images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        image.save(filename)\n",
    "        print(f\"‚úÖ Downloaded {filename}\")\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sample chest X-ray URLs (public domain)\n",
    "frontal_url = \"https://openi.nlm.nih.gov/imgs/512/145/145/CXR145_IM-0290-1001.png\"\n",
    "lateral_url = \"https://openi.nlm.nih.gov/imgs/512/145/145/CXR145_IM-0290-2001.png\"\n",
    "\n",
    "frontal_image = download_image(frontal_url, \"sample_frontal.png\")\n",
    "lateral_image = download_image(lateral_url, \"sample_lateral.png\")\n",
    "\n",
    "# Display the images\n",
    "if frontal_image:\n",
    "    print(\"\\nFrontal X-ray:\")\n",
    "    display(frontal_image.resize((256, 256)))\n",
    "    \n",
    "if lateral_image:\n",
    "    print(\"\\nLateral X-ray:\")\n",
    "    display(lateral_image.resize((256, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate-viz"
   },
   "source": "## 9. Generate Attention Visualizations (Memory-Safe)\n\n**Ultra-conservative settings to prevent OOM crashes:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-viz"
   },
   "outputs": [],
   "source": "print(\"üéØ Starting attention visualization...\")\n\n# Check if visualizer loaded successfully\nif 'visualizer' not in globals() or visualizer is None:\n    print(\"‚ùå MAIRA-2 not loaded. Please run the model loading cell first.\")\nelse:\n    check_memory()\n\n    try:\n        # Very conservative settings for 14.7GB GPU\n        output_dir, generated_report = visualizer.generate_attention_pngs(\n            frontal_image=frontal_image,\n            lateral_image=None,  # Skip lateral to save ~2GB memory\n            indication=\"Shortness of breath and chest pain\",\n            technique=\"PA chest X-ray\",\n            comparison=\"No prior studies available\",\n            max_new_tokens=10,   # Very low to prevent OOM\n            visualize_every_n=10, # Only visualize every 10th token\n            output_dir=\"colab_attention_output\"\n        )\n        \n        print(f\"\\n‚úÖ Visualizations completed!\")\n        print(f\"üìÅ Output directory: {output_dir}\")\n        print(f\"üìÑ Generated report: {generated_report}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error during visualization: {e}\")\n        print(\"This is likely a GPU memory issue.\")\n        \n        # Try emergency cleanup and retry with even smaller settings\n        clear_memory()\n        print(\"\\nüîß Trying with minimal settings...\")\n        \n        try:\n            output_dir, generated_report = visualizer.generate_attention_pngs(\n                frontal_image=frontal_image,\n                lateral_image=None,\n                indication=\"Chest pain\",\n                max_new_tokens=5,    # Absolute minimum\n                visualize_every_n=20, # Almost no visualizations\n                output_dir=\"colab_attention_output\"\n            )\n            print(f\"\\n‚úÖ Minimal visualization completed!\")\n            print(f\"üìÑ Generated report: {generated_report}\")\n            \n        except Exception as e2:\n            print(f\"‚ùå Even minimal settings failed: {e2}\")\n            print(\"\\nüí° Your GPU doesn't have enough memory for MAIRA-2.\")\n            print(\"Try:\")\n            print(\"1. Colab Pro (better GPUs)\")\n            print(\"2. Different time of day\")\n            print(\"3. Kaggle Notebooks (might have more memory)\")\n\n    check_memory()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-results"
   },
   "source": [
    "## 10. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-results"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List generated files\n",
    "output_dir = \"colab_attention_output\"\n",
    "if os.path.exists(output_dir):\n",
    "    files = os.listdir(output_dir)\n",
    "    print(f\"üìÅ Generated files ({len(files)} total):\")\n",
    "    for file in sorted(files)[:10]:  # Show first 10 files\n",
    "        print(f\"  - {file}\")\n",
    "    if len(files) > 10:\n",
    "        print(f\"  ... and {len(files) - 10} more files\")\n",
    "    \n",
    "    # Show the generated report\n",
    "    report_file = os.path.join(output_dir, \"generated_report.txt\")\n",
    "    if os.path.exists(report_file):\n",
    "        with open(report_file, 'r') as f:\n",
    "            report = f.read()\n",
    "        print(f\"\\nüìÑ Generated Report:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(report)\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Display a few attention visualizations\n",
    "    png_files = [f for f in files if f.endswith('.png')][:3]  # Show first 3\n",
    "    \n",
    "    for png_file in png_files:\n",
    "        try:\n",
    "            img_path = os.path.join(output_dir, png_file)\n",
    "            img = Image.open(img_path)\n",
    "            print(f\"\\nüéØ {png_file}:\")\n",
    "            display(img)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not display {png_file}: {e}\")\nelse:\n",
    "    print(f\"‚ùå Output directory '{output_dir}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-results"
   },
   "source": [
    "## 11. Download Results\n",
    "\n",
    "Create a zip file to download all results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-zip"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create a zip file with all results\n",
    "zip_filename = \"maira2_attention_results.zip\"\n",
    "output_dir = \"colab_attention_output\"\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "        for root, dirs, files in os.walk(output_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, os.path.relpath(file_path, '.'))\n",
    "    \n",
    "    print(f\"‚úÖ Created {zip_filename}\")\n",
    "    print(f\"üìÅ Size: {os.path.getsize(zip_filename) / 1024**2:.1f} MB\")\n",
    "    print(\"\\nüì• To download: Click the folder icon (üóÇÔ∏è) on the left, find the zip file, and download it\")\nelse:\n",
    "    print(\"‚ùå No results to zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## 12. Clean Up Memory\n",
    "\n",
    "Run this when you're done to free up memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup-memory"
   },
   "outputs": [],
   "source": [
    "# Clean up to free memory\n",
    "try:\n",
    "    del visualizer\n",
    "except:\n",
    "    pass\n",
    "\n",
    "clear_memory()\n",
    "print(\"‚úÖ Memory cleaned up\")\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": "## üîß Troubleshooting\n\n**Your GPU (14.7GB) is right at the limit for MAIRA-2 (~15GB needed).**\n\n**If you get OOM crashes:**\n\n1. **Immediate fixes:**\n   - Runtime > Restart Runtime\n   - Run cells 1-6, then try loading again\n   - Close other browser tabs to free system RAM\n\n2. **Try different times:**\n   - Early morning or late night (less Colab usage)\n   - Weekends might have better GPU availability\n\n3. **Alternative platforms:**\n   - **Kaggle Notebooks** (often more generous with memory)\n   - **Colab Pro** ($10/month - guaranteed better GPUs)\n   - **Paperspace Gradient** (free tier available)\n\n4. **Memory optimization that worked:**\n   - Only frontal X-ray (no lateral)\n   - `max_new_tokens=5` (absolute minimum)\n   - `visualize_every_n=20` (few visualizations)\n\n**Expected behavior:**\n- Model loading: Uses ~13-14GB\n- Generation: Adds 1-2GB temporarily\n- Your 14.7GB GPU is borderline - success depends on exact memory fragmentation\n\n**Success tips:**\n- Fresh runtime (no previous models loaded)\n- Minimal browser tabs open\n- Try 2-3 times if first attempt fails"
  }
 ]
}