{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# MAIRA-2 Attention Visualization on Google Colab\n",
        "\n",
        "This notebook helps you visualize attention patterns in MAIRA-2 using free GPU resources.\n",
        "\n",
        "**‚ö†Ô∏è Important Setup:**\n",
        "1. Go to **Runtime > Change runtime type**\n",
        "2. Set **Hardware accelerator** to **GPU**\n",
        "3. Click **Save**\n",
        "4. You'll need a Hugging Face token from https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "check-gpu"
      },
      "source": [
        "## 1. Check GPU Availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import subprocess\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU detected! Go to Runtime > Change runtime type > GPU\")\n",
        "\n",
        "# Check available RAM\n",
        "result = subprocess.run(['free', '-h'], capture_output=True, text=True)\n",
        "print(\"\\nSystem Memory:\")\n",
        "print(result.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-deps"
      },
      "source": [
        "## 2. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision transformers accelerate\n",
        "!pip install pillow matplotlib tqdm requests\n",
        "!pip install huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clone-repo"
      },
      "source": [
        "## 3. Get the Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/javier-alvarez/maira-interp.git\n",
        "%cd maira-interp\n",
        "\n",
        "# Verify files\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auth-setup"
      },
      "source": [
        "## 4. Set Up Authentication\n",
        "\n",
        "**You need a Hugging Face token to access MAIRA-2:**\n",
        "1. Go to https://huggingface.co/settings/tokens\n",
        "2. Create a new token with 'Read' permissions\n",
        "3. Request access to MAIRA-2: https://huggingface.co/microsoft/maira-2\n",
        "4. Enter your token in the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auth"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Enter your Hugging Face token (it will be hidden)\n",
        "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "print(\"‚úÖ Token set!\")\n",
        "\n",
        "# Test authentication\n",
        "from huggingface_hub import HfApi\n",
        "try:\n",
        "    api = HfApi()\n",
        "    user = api.whoami(token=hf_token)\n",
        "    print(f\"‚úÖ Authenticated as: {user['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Authentication failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-import"
      },
      "source": [
        "## 5. Test Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import-test"
      },
      "outputs": [],
      "source": [
        "# Test importing the visualizer\n",
        "try:\n",
        "    from attention_visualizer import MAIRA2AttentionVisualizer\n",
        "    print(\"‚úÖ Successfully imported MAIRA2AttentionVisualizer\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import failed: {e}\")\n",
        "    print(\"Current directory:\", os.getcwd())\n",
        "    print(\"Files:\", os.listdir('.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memory-opt"
      },
      "source": [
        "## 6. Memory Optimization Setup\n",
        "\n",
        "**Important:** MAIRA-2 requires ~15GB GPU memory. Free Colab GPUs have 16GB, so we need to be careful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memory-setup"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU and system memory\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def check_memory():\n",
        "    \"\"\"Check current GPU memory usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        print(f\"GPU Memory - Allocated: {allocated:.1f}GB, Reserved: {reserved:.1f}GB, Total: {total:.1f}GB\")\n",
        "        return allocated, reserved, total\n",
        "    return 0, 0, 0\n",
        "\n",
        "# Clear memory before starting\n",
        "clear_memory()\n",
        "check_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init-model"
      },
      "source": [
        "## 7. Initialize MAIRA-2 (This may take several minutes)\n",
        "\n",
        "**Note:** The first run will download ~13GB of model weights. This may take 5-10 minutes on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-init"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Initializing MAIRA-2 Attention Visualizer...\")\n",
        "print(\"This will download ~13GB of model weights on first run.\")\n",
        "print(\"Please be patient - this may take 5-10 minutes.\")\n",
        "\n",
        "try:\n",
        "    visualizer = MAIRA2AttentionVisualizer()\n",
        "    print(\"‚úÖ MAIRA-2 loaded successfully!\")\n",
        "    check_memory()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to load MAIRA-2: {e}\")\n",
        "    print(\"\\nTrying memory optimization...\")\n",
        "    clear_memory()\n",
        "    \n",
        "    # If it fails, you might need to restart runtime and try again\n",
        "    print(\"If this fails, try: Runtime > Restart Runtime and run from the beginning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample-images"
      },
      "source": [
        "## 8. Download Sample Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-images"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Download sample chest X-ray images\n",
        "def download_image(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "        image.save(filename)\n",
        "        print(f\"‚úÖ Downloaded {filename}\")\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to download {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Sample chest X-ray URLs (public domain)\n",
        "frontal_url = \"https://openi.nlm.nih.gov/imgs/512/145/145/CXR145_IM-0290-1001.png\"\n",
        "lateral_url = \"https://openi.nlm.nih.gov/imgs/512/145/145/CXR145_IM-0290-2001.png\"\n",
        "\n",
        "frontal_image = download_image(frontal_url, \"sample_frontal.png\")\n",
        "lateral_image = download_image(lateral_url, \"sample_lateral.png\")\n",
        "\n",
        "# Display the images\n",
        "if frontal_image:\n",
        "    print(\"\\nFrontal X-ray:\")\n",
        "    display(frontal_image.resize((256, 256)))\n",
        "    \n",
        "if lateral_image:\n",
        "    print(\"\\nLateral X-ray:\")\n",
        "    display(lateral_image.resize((256, 256)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generate-viz"
      },
      "source": [
        "## 9. Generate Attention Visualizations\n",
        "\n",
        "**Memory-optimized settings for Colab:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-viz"
      },
      "outputs": [],
      "source": [
        "print(\"üéØ Starting attention visualization...\")\n",
        "check_memory()\n",
        "\n",
        "try:\n",
        "    # Generate with memory-friendly settings\n",
        "    output_dir, generated_report = visualizer.generate_attention_pngs(\n",
        "        frontal_image=frontal_image,\n",
        "        lateral_image=lateral_image,  # Optional: set to None to save memory\n",
        "        indication=\"Shortness of breath and chest pain\",\n",
        "        technique=\"PA and lateral chest X-rays\",\n",
        "        comparison=\"No prior studies available\",\n",
        "        max_new_tokens=30,  # Reduced for memory\n",
        "        visualize_every_n=5,  # Only visualize every 5th token\n",
        "        output_dir=\"colab_attention_output\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úÖ Visualizations completed!\")\n",
        "    print(f\"üìÅ Output directory: {output_dir}\")\n",
        "    print(f\"üìÑ Generated report: {generated_report}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during visualization: {e}\")\n",
        "    clear_memory()\n",
        "    print(\"\\nMemory cleared. You may need to reduce max_new_tokens or restart runtime.\")\n",
        "\n",
        "check_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-results"
      },
      "source": [
        "## 10. View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show-results"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List generated files\n",
        "output_dir = \"colab_attention_output\"\n",
        "if os.path.exists(output_dir):\n",
        "    files = os.listdir(output_dir)\n",
        "    print(f\"üìÅ Generated files ({len(files)} total):\")\n",
        "    for file in sorted(files)[:10]:  # Show first 10 files\n",
        "        print(f\"  - {file}\")\n",
        "    if len(files) > 10:\n",
        "        print(f\"  ... and {len(files) - 10} more files\")\n",
        "    \n",
        "    # Show the generated report\n",
        "    report_file = os.path.join(output_dir, \"generated_report.txt\")\n",
        "    if os.path.exists(report_file):\n",
        "        with open(report_file, 'r') as f:\n",
        "            report = f.read()\n",
        "        print(f\"\\nüìÑ Generated Report:\")\n",
        "        print(\"=\" * 50)\n",
        "        print(report)\n",
        "        print(\"=\" * 50)\n",
        "    \n",
        "    # Display a few attention visualizations\n",
        "    png_files = [f for f in files if f.endswith('.png')][:3]  # Show first 3\n",
        "    \n",
        "    for png_file in png_files:\n",
        "        try:\n",
        "            img_path = os.path.join(output_dir, png_file)\n",
        "            img = Image.open(img_path)\n",
        "            print(f\"\\nüéØ {png_file}:\")\n",
        "            display(img)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Could not display {png_file}: {e}\")\nelse:\n",
        "    print(f\"‚ùå Output directory '{output_dir}' not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-results"
      },
      "source": [
        "## 11. Download Results\n",
        "\n",
        "Create a zip file to download all results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-zip"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create a zip file with all results\n",
        "zip_filename = \"maira2_attention_results.zip\"\n",
        "output_dir = \"colab_attention_output\"\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "        for root, dirs, files in os.walk(output_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                zipf.write(file_path, os.path.relpath(file_path, '.'))\n",
        "    \n",
        "    print(f\"‚úÖ Created {zip_filename}\")\n",
        "    print(f\"üìÅ Size: {os.path.getsize(zip_filename) / 1024**2:.1f} MB\")\n",
        "    print(\"\\nüì• To download: Click the folder icon (üóÇÔ∏è) on the left, find the zip file, and download it\")\nelse:\n",
        "    print(\"‚ùå No results to zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup"
      },
      "source": [
        "## 12. Clean Up Memory\n",
        "\n",
        "Run this when you're done to free up memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup-memory"
      },
      "outputs": [],
      "source": [
        "# Clean up to free memory\n",
        "try:\n",
        "    del visualizer\n",
        "except:\n",
        "    pass\n",
        "\n",
        "clear_memory()\n",
        "print(\"‚úÖ Memory cleaned up\")\n",
        "check_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## üîß Troubleshooting\n",
        "\n",
        "**Common Issues:**\n",
        "\n",
        "1. **CUDA Out of Memory:**\n",
        "   - Reduce `max_new_tokens` to 10-20\n",
        "   - Set `lateral_image=None` \n",
        "   - Restart runtime: Runtime > Restart Runtime\n",
        "\n",
        "2. **Model Download Fails:**\n",
        "   - Check your HF token has access to MAIRA-2\n",
        "   - Wait a few minutes and try again\n",
        "   - Clear cache: `!rm -rf ~/.cache/huggingface/`\n",
        "\n",
        "3. **Import Errors:**\n",
        "   - Make sure you ran the git clone cell\n",
        "   - Check you're in the right directory: `%cd maira-interp`\n",
        "\n",
        "4. **Session Timeout:**\n",
        "   - Colab free tier has 12-hour limits\n",
        "   - Download your results before the session expires\n",
        "\n",
        "**Memory Tips:**\n",
        "- Use `check_memory()` to monitor GPU usage\n",
        "- Run `clear_memory()` between experiments\n",
        "- Restart runtime if memory gets too full"
      ]
    }
  ]
}